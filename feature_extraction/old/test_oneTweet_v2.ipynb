{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run it with Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append path\n",
    "import sys\n",
    "sys.path.append('../dataset')\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import OS\n",
    "import os\n",
    "\n",
    "# Librairies for preprocessing the tweets\n",
    "import io\n",
    "import unittest\n",
    "\n",
    "# CQRI to get tweets\n",
    "from QCRI import CQRI\n",
    "import preprocessor as p\n",
    "\n",
    "# Librairies for computations and ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Functions\n",
    "import import_ipynb # Remove if you don't use Jupyter\n",
    "from featureExtraction_functions import tf_idf # Remove if you don't use Jupyter\n",
    "\n",
    "# Python utility tools \n",
    "import re # to remove URL's from tweets\n",
    "import datetime,time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract events ID\n",
    "dataset = CQRI('../twitter.txt')\n",
    "events = dataset.get_dict()   # start Jupyter with the command line: --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "                              # for ex.: ipython3 notebook --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "#print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocessor' has no attribute 'set_options'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7120710cb4aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnbrEvents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mS_list_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHASHTAG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMENTION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEMOJI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSMILEY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkeyEvent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'events/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mkeyEvent\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# check that the event file exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'preprocessor' has no attribute 'set_options'"
     ]
    }
   ],
   "source": [
    "# Training on the whole tweets dataset to learn the vocabulary\n",
    "nbrEvents = 0\n",
    "S_list_total = [];\n",
    "p.set_options(p.OPT.URL, p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "for keyEvent in events:\n",
    "    if os.path.isfile('events/'+keyEvent+'.json'):  # check that the event file exists\n",
    "        nbrEvents += 1  \n",
    "        dico = dataset.get_tweets('events/'+keyEvent+'.json')\n",
    "        # Extraction of the tweet strings and date\n",
    "        for keyTweet in dico:           #iterates over the keys\n",
    "            date,text = dico[keyTweet]\n",
    "            #print(date)\n",
    "\n",
    "            if type(text) == str and type(date) == datetime.datetime:  # because sometimes text is just nothing, so of type None\n",
    "                text = re.sub(r\"http\\S+\", \"\", text) # remove URL's\n",
    "                text = re.sub(r\"@\\S+\",\"\",text)     # Optional: remove user names\n",
    "                #text=re.sub(r\"\\\\xa0\\S+\",\"\",text)\n",
    "                text = p.clean(text)\n",
    "                S_list_total.append(text)\n",
    "            \n",
    "            \n",
    "vectorizer = TfidfVectorizer()\n",
    "dummy = vectorizer.fit(S_list_total)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count_max =  5\n",
      "Count_max =  5\n",
      "Final count =  5\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  4\n",
      "Count_max =  8\n",
      "Count_max =  6\n",
      "Final count =  8\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  12\n",
      "Final count =  12\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  2\n",
      "Count_max =  1\n",
      "Final count =  2\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  4\n",
      "Count_max =  5\n",
      "Count_max =  4\n",
      "Final count =  5\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  12\n",
      "Final count =  12\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  5\n",
      "Count_max =  5\n",
      "Final count =  5\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  1\n",
      "Count_max =  1\n",
      "Final count =  1\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  1\n",
      "Count_max =  1\n",
      "Final count =  1\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  7\n",
      "Count_max =  3\n",
      "Final count =  7\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  3\n",
      "Count_max =  2\n",
      "Final count =  3\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  9\n",
      "Count_max =  3\n",
      "Final count =  9\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  3\n",
      "Count_max =  5\n",
      "Count_max =  5\n",
      "Final count =  5\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  3\n",
      "Count_max =  1\n",
      "Final count =  3\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  1\n",
      "Count_max =  1\n",
      "Final count =  1\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  2\n",
      "Count_max =  4\n",
      "Count_max =  2\n",
      "Final count =  4\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  9\n",
      "Count_max =  4\n",
      "Final count =  9\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  8\n",
      "Count_max =  7\n",
      "Final count =  8\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  12\n",
      "Final count =  12\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  4\n",
      "Count_max =  1\n",
      "Final count =  4\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  4\n",
      "Count_max =  8\n",
      "Count_max =  7\n",
      "Final count =  8\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  3\n",
      "Count_max =  2\n",
      "Final count =  3\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  12\n",
      "Final count =  12\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  9\n",
      "Count_max =  11\n",
      "Count_max =  18\n",
      "Final count =  11\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  4\n",
      "Count_max =  4\n",
      "Final count =  4\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  3\n",
      "Count_max =  5\n",
      "Count_max =  4\n",
      "Final count =  5\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  9\n",
      "Count_max =  12\n",
      "Final count =  9\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  8\n",
      "Count_max =  7\n",
      "Final count =  8\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  2\n",
      "Count_max =  1\n",
      "Final count =  2\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  3\n",
      "Count_max =  5\n",
      "Count_max =  9\n",
      "Count_max =  5\n",
      "Final count =  9\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  1\n",
      "Count_max =  2\n",
      "Count_max =  3\n",
      "Count_max =  5\n",
      "Count_max =  2\n",
      "Final count =  5\n",
      "Final interval content:\n",
      "\n",
      "Count_max =  12\n",
      "Final count =  12\n",
      "Final interval content:\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-8283e36380e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[0mvec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;31m#print(vec)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N=12 #reference number of intervals\n",
    "K = 5000\n",
    "\n",
    "featuresTensor = []; # list containing tuples (matrixOfFeatures,label), where matrixOfFeatures is a matrix of size K x (number of time interval)\n",
    "\n",
    "for keyEvent in events:\n",
    "    if os.path.isfile('events/'+keyEvent+'.json'):  # check that the event file exists\n",
    "        dico = dataset.get_tweets('events/'+keyEvent+'.json')\n",
    "        ev = events[keyEvent]\n",
    "        label = ev[1]\n",
    "\n",
    "        S_list = [];\n",
    "        date_list = [];\n",
    "\n",
    "        # Extraction of the tweet strings and date\n",
    "        for keyTweet in dico:           #iterates over the keys\n",
    "            date,text = dico[keyTweet]\n",
    "            #print(date)\n",
    "\n",
    "            if type(text) == str and type(date) == datetime.datetime:  # because sometimes text is just nothing, so of type None\n",
    "                text = re.sub(r\"http\\S+\", \"\", text) # remove URL's\n",
    "                text = re.sub(r\"@\\S+\",\"\",text)     # Optional: remove user names\n",
    "                #text=re.sub(r\"\\\\xa0\\S+\",\"\",text)\n",
    "                if text != '':\n",
    "                    S_list.append(text)\n",
    "                    date = time.mktime(date.timetuple()) # number of seconds since 1 January 1970\n",
    "                    date_list.append(date)\n",
    "\n",
    "        # Sorting w.r.t the date values \n",
    "        idx = np.argsort(date_list)\n",
    "        date_list=np.array(date_list)\n",
    "        date_list=date_list[idx] # final list is a numpy array sorted in ascending order\n",
    "        S_list_sorted=[0]*len(S_list)\n",
    "        for i in range(0,len(S_list)):\n",
    "            S_list_sorted[i]=S_list[idx[i]]  # S_list is sorted in ascending order w.r.t the date values\n",
    "\n",
    "        # Time interval\n",
    "        timeStart = date_list[0]\n",
    "        timeEnd = date_list[-1]\n",
    "        totalTimeInterval = timeEnd - timeStart\n",
    "        timeStep = totalTimeInterval / N  # in [seconds], timeStep is the lowercase 'l' in the paper\n",
    "        timeStep_init = timeStep # save it for the cut of the intervals\n",
    "\n",
    "        if timeStep > totalTimeInterval:\n",
    "            print(\"timeStep > totalTimeInterval\")\n",
    "\n",
    "        #print(S_list_sorted)\n",
    "\n",
    "        # Dividing in intervals of duration equal to timeStep, and finding the continuous super-interval, i.e. the longest serie of intervals without empty space\n",
    "        count_save = 0\n",
    "        while(True):\n",
    "            # Dividing in intervals\n",
    "            S_list_intervals = []  # list of sublists containing tweets belonging to the same interval (each sublist is one interval)\n",
    "            n = 0 # global counter for the time steps\n",
    "            i = 0 # counter within an interval\n",
    "            timeUp = 0\n",
    "            while timeUp < timeStart + totalTimeInterval:\n",
    "                timeDown = timeStart + n * timeStep\n",
    "                timeUp = timeDown + timeStep\n",
    "\n",
    "                interval = []\n",
    "                while i < len(date_list):\n",
    "                    if date_list[i] <= timeUp and date_list[i] >= timeDown:\n",
    "                        interval.append(S_list_sorted[i])\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "                S_list_intervals.append(interval)\n",
    "                n += 1\n",
    "\n",
    "\n",
    "            #S_list_intervalsR = [x for x in S_list_intervals if x!=[]]\n",
    "\n",
    "            #print(S_list_intervals)\n",
    "\n",
    "            #max_interval = get_max_interval(S_list_intervals)\n",
    "\n",
    "\n",
    "            # Contiuous interval computation\n",
    "            continuous_intervals=[] # list of sublists of subsublists: \n",
    "                                    # each sublist is a continuous super-interval (continuous = no empty space between intervals)\n",
    "                                    # each subsublist is one interval inside the continuous super-interval\n",
    "            count_list=[] # list of the length of the continuous super-intervals\n",
    "            temp=[]\n",
    "            count=0\n",
    "            for elem in S_list_intervals: \n",
    "                if(elem==[]):\n",
    "                    if(temp!=[]):\n",
    "                        continuous_intervals.append(temp)\n",
    "                        count_list.append(count)\n",
    "                    temp=[]\n",
    "                    count=0\n",
    "                else:\n",
    "                    temp.append(elem)\n",
    "                    count+=1\n",
    "                    if(elem==S_list_intervals[-1]):\n",
    "                        continuous_intervals.append(temp)\n",
    "                        count_list.append(count)\n",
    "\n",
    "            count_list=np.array(count_list)\n",
    "            idx_max = np.argmax(count_list)\n",
    "            max_interval = continuous_intervals[idx_max]  # super-interval covering the longest time span\n",
    "            count_max = count_list[idx_max]               # number of intervals (and so time steps) in max_interval \n",
    "            print(\"Count_max = \",count_max)\n",
    "\n",
    "            if (count_max < N and count_max > count_save): # Half the time step and restart at the beginning of while(True)\n",
    "                #print('Redo')\n",
    "                timeStep = timeStep/2 # shorten the time interval by doubling N\n",
    "                count_save = count_max\n",
    "                max_interval_save = max_interval \n",
    "            else:                                         # Output max_interval and count_max\n",
    "                #print('Done')\n",
    "                if timeStep != timeStep_init:\n",
    "                    max_interval = max_interval_save # when outputting take the previous iteration result, that was the best because current iteration didn't improve\n",
    "                    print(\"Final count = \", count_save)\n",
    "                    print(\"Final interval content:\\n\")\n",
    "                    #print(max_interval_save)\n",
    "                else:\n",
    "                    print(\"Final count = \", count_max)\n",
    "                    print(\"Final interval content:\\n\")\n",
    "                    #print(max_interval)\n",
    "\n",
    "                break\n",
    "\n",
    "        # Check lengths\n",
    "        countLen_maxInterval = 0\n",
    "        countLen_S_list = 0;\n",
    "        for elem in max_interval:\n",
    "            countLen_maxInterval += len(elem)\n",
    "        for elem2 in S_list_intervals:\n",
    "            if elem2 != []:\n",
    "                countLen_S_list += len(elem2)\n",
    "        if countLen_maxInterval > countLen_S_list:\n",
    "            print(\"Problem: there shouldn't be more tweets in the biggest interval than the total number of tweets\") \n",
    "            \n",
    "        # Apply tf_idf on each interval of the super-interval\n",
    "        featuresMat = np.zeros((K,len(max_interval)))\n",
    "        #print(max_interval)\n",
    "        for ii in range(0,len(max_interval)):\n",
    "            #featuresMat[:,ii] = tf_idf(max_interval[ii],True,K)   # to modify to take each interval separately\n",
    "            tmp = vectorizer.transform(max_interval[ii])\n",
    "            array = tmp.toarray()\n",
    "            #print(array.shape)\n",
    "            vec = np.reshape(array,(array.shape[0]*array.shape[1],1))\n",
    "            vec = np.ndarray.flatten(vec)\n",
    "            vec[::-1].sort()\n",
    "            vec = vec[0:K]\n",
    "            #print(vec)\n",
    "            #print(vec.shape) \n",
    "            #vec = array[np.nonzero(array)]\n",
    "            #vec[::-1].sort()\n",
    "            #vec = np.transpose(vec[0:K])\n",
    "            featuresMat[:,ii] = vec\n",
    "        #print(featuresMat)\n",
    "\n",
    "        featuresTensor.append((featuresMat,label))\n",
    "\n",
    "        #break # TO TEST ONLY ONE EVENT\n",
    "#print(featuresTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features = tf_idf(S_list,True)   # to modify to take each interval separately\n",
    "#print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
