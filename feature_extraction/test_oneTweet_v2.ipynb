{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run it with Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from featureExtraction_functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "#Append path\n",
    "import sys\n",
    "sys.path.append('../dataset')\n",
    "sys.path.append('..')\n",
    "\n",
    "# CQRI to get tweets\n",
    "from QCRI import CQRI\n",
    "\n",
    "# Librairies for computations and ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Functions\n",
    "import import_ipynb # Remove if you don't use Jupyter\n",
    "from featureExtraction_functions import tf_idf # Remove if you don't use Jupyter\n",
    "\n",
    "# Python utility tools \n",
    "import re # to remove URL's from tweets\n",
    "import datetime,time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract events ID\n",
    "dataset = CQRI('../twitter.txt')\n",
    "events = dataset.get_dict()   # start Jupyter with the command line: --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "                              # for ex.: ipython3 notebook --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "#print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count_max =  4\n",
      "Redo\n",
      "Count_max =  3\n",
      "Done\n",
      "Not None\n",
      "Final count =  4\n",
      "Final interval content:\n",
      "\n",
      "[[' Mr Trump, I will be voting for you but, There is an issue with your new logo with the four \"T\"s, there is a hidden swastika'], [\"Trump's logo = SWASTIKA! Still want him for President of the USA? If yes, please do us both a favor - Unfriend me. \\xa0…\", 'Swastika or X marks the spot? Hmmmmmm? \\xa0', 'Is it me or does this look like the outline of a possible swastika? \\xa0', 'So another white guy shoots people on American soil and Trump unveils a logo that looks like a jumbled swastika. Happy holidays, everyone!', \"*sees logo* \\nDonald Trump... Is that... Is that a reverse swastika?  \\n\\nI'm done, America. Hahahahahahahahahahaha!!!\", \"Four T's in a square...luxurious? And it doesn't at all look like a swastika. Dude you need stop pandering to the... \\xa0\", 'Not See Logo \\xa0…pic.twitter.com/zQX5oxTl40', 'FALSE: New Trump Campaign Logo \\xa0', 'The  campaign did not unveil a swastika-like logo: \\xa0pic.twitter.com/O1fk0qJJA9', \"Snopes: Not See Logo: Donald Trump's campaign did not unveil a new logo that resembles a swastika.\\xa0\", \"urban legend: Donald Trump's campaign did not unveil a new logo that resembles a swastika. \\xa0\", 'Snopes- Not See Logo\\xa0', \"Not See Logo: Donald Trump's campaign did not unveil a new logo that resembles a swastika.\\xa0\", \"Not See Logo: Donald Trump's campaign did not unveil a new logo that resembles a swastika. \\xa0 #factorfiction\", \"Not See Logo: Donald Trump's campaign did not unveil a new logo that resembles a swastika. \\xa0 #nsfw #nsfk\", 'Brilliant satire. So believable, because the graphic is such a perfect melding of  style and tone \\xa0', 'The realDonaldTrump campaign did not unveil a swastika-like logo: \\xa0pic.twitter.com/Dta6jxkkBt', '#BadWrappingPaperPrints\\n\\nfake Trump \"swastika T\" logo'], ['As we might have guessed, Donald Dump is a conceited, ignorant, racist, borderline fascist bully, blowhard and... \\xa0'], ['*waiting for Trump to unveil his new swastika campaign logo', '\"Make America 1940\\'s Germany Again\" wouldn\\'t fit on the hat sooooo.pic.twitter.com/72weyDFJNX']]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N=12 #reference number of intervals\n",
    "\n",
    "for keyEvent in events:\n",
    "    dico = dataset.get_tweets('events/'+keyEvent+'.json')\n",
    "\n",
    "    S_list = [];\n",
    "    date_list = [];\n",
    "\n",
    "    # Extraction of the tweet strings and date\n",
    "    for keyTweet in dico:           #iterates over the keys\n",
    "        date,text = dico[keyTweet]\n",
    "        #print(date)\n",
    "\n",
    "        if type(text) == str and type(date) == datetime.datetime:  # because sometimes text is just nothing, so of type None\n",
    "            text = re.sub(r\"http\\S+\", \"\", text) # remove URL's\n",
    "            text = re.sub(r\"@\\S+\",\"\",text)     # Optional: remove user names\n",
    "            #text=re.sub(r\"\\\\xa0\\S+\",\"\",text)\n",
    "            S_list.append(text)\n",
    "\n",
    "            date = time.mktime(date.timetuple()) # number of seconds since 1 January 1970\n",
    "            date_list.append(date)\n",
    "\n",
    "    # Sorting w.r.t the date values \n",
    "    idx = np.argsort(date_list)\n",
    "    date_list=np.array(date_list)\n",
    "    date_list=date_list[idx] # final list is a numpy array sorted in ascending order\n",
    "    S_list_sorted=[0]*len(S_list)\n",
    "    for i in range(0,len(S_list)):\n",
    "        S_list_sorted[i]=S_list[idx[i]]  # S_list is sorted in ascending order w.r.t the date values\n",
    "        \n",
    "    # Time interval\n",
    "    timeStart = date_list[0]\n",
    "    timeEnd = date_list[-1]\n",
    "    totalTimeInterval = timeEnd - timeStart\n",
    "    timeStep = totalTimeInterval / N  # in [seconds], timeStep is the lowercase 'l' in the paper\n",
    "    timeStep_init = timeStep # save it for the cut of the intervals\n",
    "    \n",
    "    if timeStep > totalTimeInterval:\n",
    "        print(\"timeStep > totalTimeInterval\")\n",
    "    \n",
    "    #print(S_list_sorted)\n",
    "\n",
    "    # Dividing in intervals of duration equal to timeStep, and finding the continuous super-interval, i.e. the longest serie of intervals without empty space\n",
    "    count_save = 0\n",
    "    while(True):\n",
    "        # Dividing in intervals\n",
    "        S_list_intervals = []  # list of sublists containing tweets belonging to the same interval (each sublist is one interval)\n",
    "        n = 0 # global counter for the time steps\n",
    "        i = 0 # counter within an interval\n",
    "        timeUp = 0\n",
    "        while timeUp < timeStart + totalTimeInterval:\n",
    "            timeDown = timeStart + n * timeStep\n",
    "            timeUp = timeDown + timeStep\n",
    "\n",
    "            interval = []\n",
    "            while i < len(date_list):\n",
    "                if date_list[i] <= timeUp and date_list[i] >= timeDown:\n",
    "                    interval.append(S_list_sorted[i])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "            S_list_intervals.append(interval)\n",
    "            n += 1\n",
    "\n",
    "        \n",
    "        #S_list_intervalsR = [x for x in S_list_intervals if x!=[]]\n",
    "        \n",
    "        #print(S_list_intervals)\n",
    "        \n",
    "        #max_interval = get_max_interval(S_list_intervals)\n",
    "\n",
    "\n",
    "        # Contiuous interval computation\n",
    "        continuous_intervals=[] # list of sublists of subsublists: \n",
    "                                # each sublist is a continuous super-interval (continuous = no empty space between intervals)\n",
    "                                # each subsublist is one interval inside the continuous super-interval\n",
    "        count_list=[] # list of the length of the continuous super-intervals\n",
    "        temp=[]\n",
    "        count=0\n",
    "        for elem in S_list_intervals: \n",
    "            if(elem==[]):\n",
    "                if(temp!=[]):\n",
    "                    continuous_intervals.append(temp)\n",
    "                    count_list.append(count)\n",
    "                temp=[]\n",
    "                count=0\n",
    "            else:\n",
    "                temp.append(elem)\n",
    "                count+=1\n",
    "                if(elem==S_list_intervals[-1]):\n",
    "                    continuous_intervals.append(temp)\n",
    "                    count_list.append(count)\n",
    "\n",
    "        count_list=np.array(count_list)\n",
    "        idx_max = np.argmax(count_list)\n",
    "        max_interval = continuous_intervals[idx_max]  # super-interval covering the longest time span\n",
    "        count_max = count_list[idx_max]               # number of intervals (and so time steps) in max_interval \n",
    "        print(\"Count_max = \",count_max)\n",
    "\n",
    "        if (count_max < N and count_max > count_save): # Half the time step and restart at the beginning of while(True)\n",
    "            #print('Redo')\n",
    "            timeStep = timeStep/2 # shorten the time interval by doubling N\n",
    "            count_save = count_max\n",
    "            max_interval_save = max_interval \n",
    "        else:                                         # Output max_interval and count_max\n",
    "            #print('Done')\n",
    "            if timeStep != timeStep_init:\n",
    "                max_interval = max_interval_save # when outputting take the previous iteration result, that was the best because current iteration didn't improve\n",
    "                print(\"Final count = \", count_save)\n",
    "                print(\"Final interval content:\\n\")\n",
    "                print(max_interval_save)\n",
    "            else:\n",
    "                print(\"Final count = \", count_max)\n",
    "                print(\"Final interval content:\\n\")\n",
    "                print(max_interval)\n",
    "                \n",
    "            break\n",
    "    \n",
    "    # Check lengths\n",
    "    countLen_maxInterval = 0\n",
    "    countLen_S_list = 0;\n",
    "    for elem in max_interval:\n",
    "        countLen_maxInterval += len(elem)\n",
    "    for elem2 in S_list_intervals:\n",
    "        if elem2 != []:\n",
    "            countLen_S_list += len(elem2)\n",
    "    if countLen_maxInterval > countLen_S_list:\n",
    "        print(\"Problem: there shouldn't be more tweets in the biggest interval than the total number of tweets\")\n",
    "    \n",
    "    # Apply tf_idf on each interval of the super-interval\n",
    "    K = 5\n",
    "    featuresMat = np.zeros((K,len(max_interval)))\n",
    "    #for ii in range(0,len(max_interval)):\n",
    "        #featuresMat[:,ii] = tf_idf(max_interval[ii],True,K)   # to modify to take each interval separately\n",
    "    \n",
    "    #print(featuresMat)\n",
    "    \n",
    "    break # TO TEST ONLY ONE EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features = tf_idf(S_list,True)   # to modify to take each interval separately\n",
    "#print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
